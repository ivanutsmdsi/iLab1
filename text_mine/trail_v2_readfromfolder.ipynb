{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1) read in file\n",
    "2) custom stop list\n",
    "2.1) prefix words from pubmed ['conclusions', 'introduction', 'methods', 'purpose', 'results']\n",
    "2.2) remove words by frequency (not apply yet, still looking for abnormality)\n",
    "3) steming & remove stop words, remove numberic, remove puntuation\n",
    "4) Apply POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1) get .csv directly from downloaded file\n",
    "import os\n",
    "import pandas as pd\n",
    "path = os.getcwd()\n",
    "pardir_path = os.path.abspath(os.path.join(path, os.pardir))\n",
    "raw_name = 'news_data_9600'\n",
    "input_folder_ = 'tina_test'\n",
    "output_folder_ = 'text_mine_output'\n",
    "format_ = '.csv'\n",
    "raw_file_path = os.path.join(pardir_path, input_folder_,(raw_name+format_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(raw_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                              Title  \\\n",
      "0           1  Construction safety blitz on both sides of the...   \n",
      "1           2  CBD apartment construction shut down over 'min...   \n",
      "2           3  Construction of continuing care facility halte...   \n",
      "3           4  Frightening drop in construction safety inspec...   \n",
      "4           5  How inspections can improve construction site ...   \n",
      "\n",
      "                                              Source  \\\n",
      "0  {'href': 'https://www.safework.nsw.gov.au', 't...   \n",
      "1  {'href': 'https://www.illawarramercury.com.au'...   \n",
      "2  {'href': 'https://www.thesafetymag.com', 'titl...   \n",
      "3  {'href': 'https://www.unitetheunion.org', 'tit...   \n",
      "4  {'href': 'https://www.thesafetymag.com', 'titl...   \n",
      "\n",
      "                       Published  \\\n",
      "0  Tue, 16 Aug 2022 07:00:00 GMT   \n",
      "1  Tue, 30 Aug 2022 07:59:15 GMT   \n",
      "2  Tue, 30 Aug 2022 16:37:21 GMT   \n",
      "3  Thu, 04 Aug 2022 07:00:00 GMT   \n",
      "4  Wed, 03 Aug 2022 07:00:00 GMT   \n",
      "\n",
      "                                             Summary  \n",
      "0  Construction safety blitz on both sides of the...  \n",
      "1  CBD apartment construction shut down over 'min...  \n",
      "2  Construction of continuing care facility halte...  \n",
      "3  Frightening drop in construction safety inspec...  \n",
      "4  How inspections can improve construction site ...  \n",
      "raw data shape is (9600, 5)\n",
      "Index(['Unnamed: 0', 'Title', 'Source', 'Published', 'Summary'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.head(5))\n",
    "print (f'raw data shape is {df.shape}') #(9600, 5)\n",
    "print(df.columns) # ['Unnamed: 0', 'Title', 'Source', 'Published', 'Summary']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.1) add index record number\n",
    "import numpy as np\n",
    "df['rec_id'] = np.arange(1, df.shape[0] + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      2022-08-16 17:00:00+10:00\n",
      "1      2022-08-30 17:59:15+10:00\n",
      "2      2022-08-31 02:37:21+10:00\n",
      "3      2022-08-04 17:00:00+10:00\n",
      "4      2022-08-03 17:00:00+10:00\n",
      "                  ...           \n",
      "9595   2022-03-16 18:00:00+11:00\n",
      "9596   2022-08-24 20:16:54+10:00\n",
      "9597   2022-08-31 12:59:07+10:00\n",
      "9598   2022-08-28 01:16:00+10:00\n",
      "9599   2022-08-30 20:03:17+10:00\n",
      "Name: Published, Length: 9600, dtype: datetime64[ns, Australia/Sydney]\n"
     ]
    }
   ],
   "source": [
    "# 1.2) convert time zone from GMT to sydney GMT+10\n",
    "df['Published']  = pd.to_datetime(df['Published']).dt.tz_convert('Australia/Sydney')\n",
    "print (df['Published'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Title</th>\n",
       "      <th>Source</th>\n",
       "      <th>Published</th>\n",
       "      <th>Summary</th>\n",
       "      <th>rec_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Construction safety blitz on both sides of the...</td>\n",
       "      <td>{'href': 'https://www.safework.nsw.gov.au', 't...</td>\n",
       "      <td>2022-08-16 17:00:00+10:00</td>\n",
       "      <td>Construction safety blitz on both sides of the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>CBD apartment construction shut down over 'min...</td>\n",
       "      <td>{'href': 'https://www.illawarramercury.com.au'...</td>\n",
       "      <td>2022-08-30 17:59:15+10:00</td>\n",
       "      <td>CBD apartment construction shut down over 'min...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Construction of continuing care facility halte...</td>\n",
       "      <td>{'href': 'https://www.thesafetymag.com', 'titl...</td>\n",
       "      <td>2022-08-31 02:37:21+10:00</td>\n",
       "      <td>Construction of continuing care facility halte...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Frightening drop in construction safety inspec...</td>\n",
       "      <td>{'href': 'https://www.unitetheunion.org', 'tit...</td>\n",
       "      <td>2022-08-04 17:00:00+10:00</td>\n",
       "      <td>Frightening drop in construction safety inspec...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>How inspections can improve construction site ...</td>\n",
       "      <td>{'href': 'https://www.thesafetymag.com', 'titl...</td>\n",
       "      <td>2022-08-03 17:00:00+10:00</td>\n",
       "      <td>How inspections can improve construction site ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              Title  \\\n",
       "0           1  Construction safety blitz on both sides of the...   \n",
       "1           2  CBD apartment construction shut down over 'min...   \n",
       "2           3  Construction of continuing care facility halte...   \n",
       "3           4  Frightening drop in construction safety inspec...   \n",
       "4           5  How inspections can improve construction site ...   \n",
       "\n",
       "                                              Source  \\\n",
       "0  {'href': 'https://www.safework.nsw.gov.au', 't...   \n",
       "1  {'href': 'https://www.illawarramercury.com.au'...   \n",
       "2  {'href': 'https://www.thesafetymag.com', 'titl...   \n",
       "3  {'href': 'https://www.unitetheunion.org', 'tit...   \n",
       "4  {'href': 'https://www.thesafetymag.com', 'titl...   \n",
       "\n",
       "                  Published  \\\n",
       "0 2022-08-16 17:00:00+10:00   \n",
       "1 2022-08-30 17:59:15+10:00   \n",
       "2 2022-08-31 02:37:21+10:00   \n",
       "3 2022-08-04 17:00:00+10:00   \n",
       "4 2022-08-03 17:00:00+10:00   \n",
       "\n",
       "                                             Summary  rec_id  \n",
       "0  Construction safety blitz on both sides of the...       1  \n",
       "1  CBD apartment construction shut down over 'min...       2  \n",
       "2  Construction of continuing care facility halte...       3  \n",
       "3  Frightening drop in construction safety inspec...       4  \n",
       "4  How inspections can improve construction site ...       5  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardised column names : Index(['Unnamed: 0', 'title', 'source', 'published_date', 'abstract',\n",
      "       'rec_id'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# 1.3) rename columns\n",
    "df.rename(columns={ 'Title':'title', 'Source':'source','Published': 'published_date', 'Summary':'abstract'}, inplace=True)\n",
    "print(f'Standardised column names : {df.columns}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\TinaM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\TinaM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\TinaM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\TinaM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\TinaM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## 2. Create stopword list\n",
    "#2) Prep file\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4') # use in stemming\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 188 words in the default stop words list.\n",
      "{'had', \"didn't\", 'this', 'm', 'how', 'both', 'won', 'will', 'why', 'has', 'them', 't', 'she', 'over', 'again', 'objective:', 'what', 'was', 'hasn', 'my', 'themselves', \"should've\", 'conclusions:', 'is', 'our', 'been', \"it's\", 'didn', 'they', \"haven't\", 'o', 'mightn', 'd', 'a', 'hers', 've', 'where', \"wasn't\", 'i', 'other', 'some', 'about', 'yours', 'no', \"mustn't\", 'am', \"hasn't\", 'your', 'we', 'shouldn', 'those', 'in', 'who', 's', 'very', 'wouldn', 'him', 'there', 'which', 'but', 'needn', 'while', 'introduction:', 'ma', 'and', 'below', \"hadn't\", 'as', 'couldn', 'own', 'or', 'll', 'herself', 'me', 'most', 'background:', 'at', 'from', 'few', 'do', 'once', 'their', \"mightn't\", 'being', 'same', 'now', 'under', \"you'll\", \"aren't\", \"shouldn't\", 'whom', 'doing', 'his', 'haven', 'isn', 'were', 'purpose:', \"that'll\", \"isn't\", 'if', 'himself', 'yourself', 'such', 'theirs', 'for', 'an', \"you're\", \"you've\", \"shan't\", \"wouldn't\", 'he', 'because', 'off', 'after', 'wasn', 'only', 'when', 'yourselves', 'br', 'before', 'more', 'with', 'further', 'should', 'too', 'that', 'against', 'than', 'you', 'so', \"don't\", 'between', 'the', \"needn't\", 'by', 'href', 'each', 'did', 'through', 'during', 'on', 'myself', 'shan', 'hadn', 'does', \"couldn't\", 'methods:', 'ours', 'have', 'aren', 'ain', 'can', 'itself', 'down', 'be', 'any', 'until', 'of', 'its', 'above', 'nor', 'all', 'weren', \"weren't\", 're', 'doesn', \"won't\", 'just', 'ourselves', 'results:', 'then', 'not', 'y', 'to', 'into', 'these', \"she's\", 'here', 'out', 'her', 'are', 'mustn', \"you'd\", 'don', \"doesn't\", 'up', 'having', 'it'}\n"
     ]
    }
   ],
   "source": [
    "# 2.1) view the default stopword list:\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "pub_med_cust_stop = ['conclusions:', 'introduction:', 'methods:', 'purpose:', 'results:', 'objective:','background:']\n",
    "\n",
    "stops.update(['br', 'href'],pub_med_cust_stop)\n",
    "\n",
    "print(f'There are {len(stops)} words in the default stop words list.')\n",
    "print(stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2) Create another customer stopword list from the word frequency\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       Construction safety blitz on both sides of the...\n",
       "1       CBD apartment construction shut down over 'min...\n",
       "2       Construction of continuing care facility halte...\n",
       "3       Frightening drop in construction safety inspec...\n",
       "4       How inspections can improve construction site ...\n",
       "                              ...                        \n",
       "9595    Joint safety blitz aims to cut falls in constr...\n",
       "9596    How Tideway is raising the bar for safety trai...\n",
       "9597    Broughton Street business owners discuss impac...\n",
       "9598    Nearly $51 mil awarded to county jail renovati...\n",
       "9599    This much I know: 'I'm determined to break dow...\n",
       "Name: abstract, Length: 9600, dtype: object"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['abstract']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TinaM\\AppData\\Local\\Temp/ipykernel_21244/1362433962.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  custom_abstract = df['abstract'].str.replace(r'[0-9]+', '').apply(lambda words: ' '.join(word.lower() for word in words.split() if word not in stops))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract after remvoing numeric and default stopwords\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0                                            sides border\n",
       "1                  cbd shut 'minefield' illawarra mercury\n",
       "2                                         care halted due\n",
       "3                    frightening drop exposed unite union\n",
       "4                                                        \n",
       "                              ...                        \n",
       "9595                                            joint cut\n",
       "9596                                          tideway bar\n",
       "9597    broughton owners discuss impact working foxmed...\n",
       "9598                       nearly $ mil awarded jail wtvg\n",
       "9599           much know: 'i'm determined break barriers'\n",
       "Name: abstract, Length: 9600, dtype: object"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## remove stopwords from the abstract to see find the Tfidf\n",
    "custom_abstract = df['abstract'].str.replace(r'[0-9]+', '')\n",
    "custom_abstract =custom_abstract.apply(lambda words: ' '.join(word.lower() for word in words.split() if word not in stops))\n",
    "\n",
    "print('Abstract after remvoing numeric and default stopwords')\n",
    "custom_abstract "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abstract after remvoing numeric and default stopwords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TinaM\\AppData\\Local\\Temp/ipykernel_21244/3491161362.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  custom_title= df['title'].str.replace(r'[0-9]+', '')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       Construction safety blitz on both sides of the...\n",
       "1       CBD apartment construction shut down over 'min...\n",
       "2       Construction of continuing care facility halte...\n",
       "3       Frightening drop in construction safety inspec...\n",
       "4       How inspections can improve construction site ...\n",
       "                              ...                        \n",
       "9595    Joint safety blitz aims to cut falls in constr...\n",
       "9596    How Tideway is raising the bar for safety trai...\n",
       "9597    Broughton Street business owners discuss impac...\n",
       "9598    Nearly $ mil awarded to county jail renovation...\n",
       "9599    This much I know: 'I'm determined to break dow...\n",
       "Name: title, Length: 9600, dtype: object"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## remove stopwords from the title to see find the Tfidf\n",
    "custom_title= df['title'].str.replace(r'[0-9]+', '')\n",
    "\n",
    "print('Abstract after remvoing numeric and default stopwords')\n",
    "custom_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF table.\n",
      "              idf_weights\n",
      "construction     1.437479\n",
      "safety           1.506426\n",
      "magazine         3.260868\n",
      "health           3.273940\n",
      "new              3.321617\n",
      "...                   ...\n",
      "says             9.071010\n",
      "daily            9.071010\n",
      "urgent           9.071010\n",
      "opseu            9.071010\n",
      "commercial       9.071010\n",
      "\n",
      "[661 rows x 1 columns]\n",
      "Distribution of the idf_weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:388: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['background', 'conclusions', 'introduction', 'methods', 'objective', 'purpose', 'results'] not in stop_words.\n",
      "  warnings.warn('Your stop_words may be inconsistent with '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    661.000000\n",
       "mean       5.556095\n",
       "std        0.812163\n",
       "min        1.437479\n",
       "25%        5.554502\n",
       "50%        5.554502\n",
       "75%        5.554502\n",
       "max        9.071010\n",
       "Name: idf_weights, dtype: float64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## initiate CountVectorizer\n",
    "uni_count = CountVectorizer(stop_words=list(stops) + pub_med_cust_stop\n",
    "                           ,ngram_range=(1,1) \n",
    "                           )\n",
    "\n",
    "uni_word_count=uni_count.fit_transform(custom_abstract)\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(uni_word_count)\n",
    "## keep words in pub_med_cust_stop list if it's not follow by ':', as they might be part of the real text\n",
    "\n",
    "### idf value\n",
    "df_idf = pd.DataFrame(tfidf_transformer.idf_, index=uni_count.get_feature_names(),columns=[\"idf_weights\"])\n",
    "\n",
    "print(f'IDF table - Abstract')\n",
    "print(df_idf.sort_values(by=['idf_weights']))\n",
    "\n",
    "print('Distribution of the idf_weights')\n",
    "df_idf.idf_weights.describe()\n",
    "## the lowest weight is 'cont'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words with idf_weights less than 25% across all docs, total 121 words.\n",
      "              idf_weights\n",
      "construction     1.437479\n",
      "safety           1.506426\n",
      "magazine         3.260868\n",
      "health           3.273940\n",
      "new              3.321617\n",
      "...                   ...\n",
      "protect          5.357438\n",
      "help             5.433424\n",
      "tech             5.525232\n",
      "latest           5.534894\n",
      "sun              5.534894\n",
      "\n",
      "[121 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "## after removing the \n",
    "idf_stop = df_idf.idf_weights.describe()[5] ## idf_weights at 25%\n",
    "\n",
    "print(f'words with idf_weights less than 25% across all docs, total {len(df_idf[df_idf.idf_weights<idf_stop])} words.')\n",
    "print(df_idf[df_idf.idf_weights<idf_stop].sort_values(by=['idf_weights']))\n",
    "\n",
    "idf_stop_list = list(df_idf[df_idf.idf_weights<idf_stop].sort_values(by=['idf_weights']).index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 309 final stop words for abstract.\n",
      "['had', \"didn't\", 'this', 'm', 'how', 'both', 'won', 'will', 'why', 'has', 'them', 't', 'she', 'over', 'again', 'objective:', 'what', 'was', 'hasn', 'my', 'themselves', \"should've\", 'conclusions:', 'is', 'our', 'been', \"it's\", 'didn', 'they', \"haven't\", 'o', 'mightn', 'd', 'a', 'hers', 've', 'where', \"wasn't\", 'i', 'other', 'some', 'about', 'yours', 'no', \"mustn't\", 'am', \"hasn't\", 'your', 'we', 'shouldn', 'those', 'in', 'who', 's', 'very', 'wouldn', 'him', 'there', 'which', 'but', 'needn', 'while', 'introduction:', 'ma', 'and', 'below', \"hadn't\", 'as', 'couldn', 'own', 'or', 'll', 'herself', 'me', 'most', 'background:', 'at', 'from', 'few', 'do', 'once', 'their', \"mightn't\", 'being', 'same', 'now', 'under', \"you'll\", \"aren't\", \"shouldn't\", 'whom', 'doing', 'his', 'haven', 'isn', 'were', 'purpose:', \"that'll\", \"isn't\", 'if', 'himself', 'yourself', 'such', 'theirs', 'for', 'an', \"you're\", \"you've\", \"shan't\", \"wouldn't\", 'he', 'because', 'off', 'after', 'wasn', 'only', 'when', 'yourselves', 'br', 'before', 'more', 'with', 'further', 'should', 'too', 'that', 'against', 'than', 'you', 'so', \"don't\", 'between', 'the', \"needn't\", 'by', 'href', 'each', 'did', 'through', 'during', 'on', 'myself', 'shan', 'hadn', 'does', \"couldn't\", 'methods:', 'ours', 'have', 'aren', 'ain', 'can', 'itself', 'down', 'be', 'any', 'until', 'of', 'its', 'above', 'nor', 'all', 'weren', \"weren't\", 're', 'doesn', \"won't\", 'just', 'ourselves', 'results:', 'then', 'not', 'y', 'to', 'into', 'these', \"she's\", 'here', 'out', 'her', 'are', 'mustn', \"you'd\", 'don', \"doesn't\", 'up', 'having', 'it', 'construction', 'safety', 'magazine', 'health', 'new', 'news', 'com', 'project', 'occupational', 'road', 'today', 'workers', 'nsw', 'canadian', 'improve', 'safework', 'management', 'osha', 'abc', 'work', 'ehs', 'engineering', 'bridge', 'traffic', 'building', 'nuclear', 'pedestrian', 'concerns', 'site', 'sites', 'begins', 'infrastructure', 'blitz', 'business', 'issues', 'dive', 'worker', 'industry', 'street', 'report', 'labor', 'gov', 'record', 'department', 'pros', 'training', 'injured', 'npp', 'notice', 'continuing', 'plan', 'offers', 'county', 'raising', 'renovations', 'among', 'ahead', 'september', 'spotlight', 'standards', 'state', 'swing', 'times', 'advisory', 'viaduct', 'administration', 'raises', 'aims', 'north', 'zone', 'green', 'guide', 'fined', 'need', 'high', 'committee', 'improvements', 'financial', 'improving', 'falls', 'global', 'international', 'inspections', 'express', 'million', 'meet', 'crash', 'market', 'facility', 'workplace', 'fox', 'digital', 'know', 'journal', 'calls', 'first', 'government', 'herald', 'development', 'us', 'tower', 'sydney', 'morning', 'apartment', 'measures', 'regulator', 'services', 'partner', 'power', 'plant', 'rtl', 'update', 'heat', 'industrial', 'ensure', 'texas', 'protect', 'help', 'tech', 'latest', 'sun']\n"
     ]
    }
   ],
   "source": [
    "## whether add to custom stop list\n",
    "_to_add_stop_words = 'N'\n",
    "\n",
    "if _to_add_stop_words == 'Y':\n",
    "    stops_abs = list(stops) + list(idf_stop_list)\n",
    "else:\n",
    "    stops_abs = stops\n",
    "\n",
    "print(f'There are {len(stops_abs)} final stop words for abstract.')\n",
    "print(stops_abs)\n",
    "## not to add low idf weigth words as want to keep 'construction', 'safety'\n",
    "\n",
    "## output 1 A - stoplist for abstract\n",
    "pd.DataFrame(list(stops_abs)).to_csv('final_stop_words_abstract.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF table - Title\n",
      "              idf_weights\n",
      "construction     1.437479\n",
      "safety           1.506426\n",
      "magazine         3.260868\n",
      "health           3.273940\n",
      "new              3.321617\n",
      "...                   ...\n",
      "says             9.071010\n",
      "daily            9.071010\n",
      "urgent           9.071010\n",
      "opseu            9.071010\n",
      "commercial       9.071010\n",
      "\n",
      "[661 rows x 1 columns]\n",
      "Distribution of the idf_weights\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    661.000000\n",
       "mean       5.556095\n",
       "std        0.812163\n",
       "min        1.437479\n",
       "25%        5.554502\n",
       "50%        5.554502\n",
       "75%        5.554502\n",
       "max        9.071010\n",
       "Name: idf_weights, dtype: float64"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#appy the same to title, initiate countervetorizer\n",
    "\n",
    "title_uni_word_count=uni_count.fit_transform(custom_title)\n",
    "title_tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "title_tfidf_transformer.fit(title_uni_word_count)\n",
    "## keep words in pub_med_cust_stop list if it's not follow by ':', as they might be part of the real text\n",
    "\n",
    "### idf value\n",
    "title_df_idf = pd.DataFrame(title_tfidf_transformer.idf_, index=uni_count.get_feature_names(),columns=[\"idf_weights\"])\n",
    "\n",
    "print(f'IDF table - Title')\n",
    "print(title_df_idf.sort_values(by=['idf_weights']))\n",
    "\n",
    "print('Distribution of the idf_weights')\n",
    "title_df_idf.idf_weights.describe()\n",
    "## the lowest weight is 'cont'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words with idf_weights less than 25% across all titles, total 121 words.\n",
      "              idf_weights\n",
      "construction     1.437479\n",
      "safety           1.506426\n",
      "magazine         3.260868\n",
      "health           3.273940\n",
      "new              3.321617\n",
      "...                   ...\n",
      "protect          5.357438\n",
      "help             5.433424\n",
      "tech             5.525232\n",
      "latest           5.534894\n",
      "sun              5.534894\n",
      "\n",
      "[121 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "## after removing the \n",
    "title_idf_stop = title_df_idf.idf_weights.describe()[5] ## idf_weights at 25%\n",
    "\n",
    "print(f'words with idf_weights less than 25% across all titles, total {len(title_df_idf[title_df_idf.idf_weights<idf_stop])} words.')\n",
    "print(title_df_idf[title_df_idf.idf_weights<idf_stop].sort_values(by=['idf_weights']))\n",
    "\n",
    "title_idf_stop_list = list(title_df_idf[title_df_idf.idf_weights<idf_stop].sort_values(by=['idf_weights']).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 0 final stop words for title.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## whether add to custom stop list in title\n",
    "_to_add_stop_words_title = 'N'\n",
    "\n",
    "if _to_add_stop_words_title == 'Y':\n",
    "    stops_title = list(idf_stop_list)\n",
    "else:\n",
    "    stops_title = ''\n",
    "\n",
    "print(f'There are {len(stops_title)} final stop words for title.')\n",
    "print(stops_title)\n",
    "\n",
    "## output 1 B - stoplist for title\n",
    "pd.DataFrame(list(stops_title)).to_csv('final_stop_words_title.csv', index=True)\n",
    "## not to add low idf weigth words as want to keep 'construction', 'safety'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Steming - abstract\n",
    "stops = stops_abs\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "# POS (Parts Of Speech) for: NN = nouns, JJ = adjectives, VB= verbs and RB= adverbs, JJR = adjective, comparative (larger), WRB=wh- adverb (how), WP = wh- pronoun (who), WDT = wh-determiner (that, what)\n",
    "DI_POS_TYPES = {'NN':'n', 'JJ':'a', 'VB':'v', 'RB':'r', 'JJR':'c', 'WRB':'b','WP':'p','WDT':'t'} \n",
    "POS_TYPES = list(DI_POS_TYPES.keys())\n",
    "\n",
    "# Constraints on tokens\n",
    "MIN_STR_LEN = 3\n",
    "RE_VALID = '[a-zA-Z]' ## only keep alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 9600 entries.\n",
      "The first entry of abstracts\n",
      "Construction safety blitz on both sides of the border  SafeWork NSW\n",
      "The second entry of abstracts\n",
      "CBD apartment construction shut down over 'minefield' of safety issues  Illawarra Mercury\n"
     ]
    }
   ],
   "source": [
    "# convcert the text column to analysis to a list\n",
    "abstracts= df.abstract.tolist()\n",
    "\n",
    "print(f'Total of {len(abstracts)} entries.')\n",
    "\n",
    "print('The first entry of abstracts')\n",
    "print(abstracts[0])\n",
    "\n",
    "print('The second entry of abstracts')\n",
    "print(abstracts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.1) set up functions to remove accents\n",
    "import unicodedata ## for remove accents function\n",
    "import string ## for remove accents function\n",
    "import re ## to remove character not in RE_VALID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized words in the first 5 entries:\n",
      "             0            1             2             3             4            5          6         7             8         9         10            11       12    13    14    15    16    17    18    19    20    21    22    23    24    25    26\n",
      "0  construction       safety         blitz            on          both        sides         of       the        border  safework       nsw          None     None  None  None  None  None  None  None  None  None  None  None  None  None  None  None\n",
      "1           cbd    apartment  construction          shut          down         over  minefield                      of    safety    issues     illawarra  mercury  None  None  None  None  None  None  None  None  None  None  None  None  None  None\n",
      "2  construction           of    continuing          care      facility       halted        due        to        safety  concerns  canadian  occupational   safety  None  None  None  None  None  None  None  None  None  None  None  None  None  None\n",
      "3   frightening         drop            in  construction        safety  inspections    exposed     unite           the     union      None          None     None  None  None  None  None  None  None  None  None  None  None  None  None  None  None\n",
      "4           how  inspections           can       improve  construction         site     safety  canadian  occupational    safety      None          None     None  None  None  None  None  None  None  None  None  None  None  None  None  None  None\n",
      "\n",
      "\n",
      "Tokenized, stemed and lemmatized first 5 entries:\n",
      "                                              lem quote\n",
      "0                         - - - - - side - - border - -\n",
      "1  cbd - - shut - - minefield - - - - illawarra mercury\n",
      "2                     - - - care - halt due - - - - - -\n",
      "3            frighten drop - - - - expose unite - union\n",
      "4                                   - - - - - - - - - -\n"
     ]
    }
   ],
   "source": [
    "## function from https://colab.research.google.com/github/gal-a/blog/blob/master/docs/notebooks/nlp/nltk_preprocess.ipynb#scrollTo=-44aMwUcQZxm\n",
    "# Remove accents function\n",
    "def remove_accents(data):\n",
    "    return ''.join(x for x in unicodedata.normalize('NFKD', data) if x in string.ascii_letters or x == \" \")\n",
    "\n",
    "# Process all quotes\n",
    "li_tokens = []\n",
    "li_token_lists = []\n",
    "li_lem_strings = []\n",
    "\n",
    "for i,text in enumerate(abstracts):\n",
    "    # Tokenize by sentence, then by lowercase word\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "\n",
    "    # Process all tokens per quote\n",
    "    li_tokens_quote = []\n",
    "    li_tokens_quote_lem = []\n",
    "    for token in tokens:\n",
    "        # Remove accents\n",
    "        t = remove_accents(token)\n",
    "\n",
    "        # Remove punctuation\n",
    "        t = str(t).translate(string.punctuation)\n",
    "        li_tokens_quote.append(t)\n",
    "        \n",
    "        # Add token that represents \"no lemmatization match\"\n",
    "        li_tokens_quote_lem.append(\"-\") # this token will be removed if a lemmatization match is found below\n",
    "\n",
    "        # Process each token\n",
    "        if t not in stops:\n",
    "            if re.search(RE_VALID, t):\n",
    "                if len(t) >= MIN_STR_LEN:\n",
    "                    # Note that the POS (Part Of Speech) is necessary as input to the lemmatizer \n",
    "                    # (otherwise it assumes the word is a noun)\n",
    "                    pos = nltk.pos_tag([t])[0][1][:2]\n",
    "                    pos2 = 'n'  # set default to noun\n",
    "                    if pos in DI_POS_TYPES:\n",
    "                      pos2 = DI_POS_TYPES[pos]\n",
    "                    \n",
    "                    stem = stemmer.stem(t)\n",
    "                    lem = lemmatizer.lemmatize(t, pos=pos2)  # lemmatize with the correct POS\n",
    "                    \n",
    "                    if pos in POS_TYPES:\n",
    "                        li_tokens.append((t, stem, lem, pos))\n",
    "\n",
    "                        # Remove the \"-\" token and append the lemmatization match\n",
    "                        li_tokens_quote_lem = li_tokens_quote_lem[:-1] \n",
    "                        li_tokens_quote_lem.append(lem)\n",
    "\n",
    "    # Build list of token lists from lemmatized tokens\n",
    "    li_token_lists.append(li_tokens_quote)\n",
    "    \n",
    "    # Build list of strings from lemmatized tokens\n",
    "    str_li_tokens_quote_lem = ' '.join(li_tokens_quote_lem)\n",
    "    li_lem_strings.append(str_li_tokens_quote_lem)\n",
    "    \n",
    "# Build resulting dataframes from lists\n",
    "df_token_lists = pd.DataFrame(li_token_lists) # (100, 549)\n",
    "\n",
    "print(\"Tokenized words in the first 5 entries:\")\n",
    "print(df_token_lists.head(5).to_string())\n",
    "\n",
    "# Replace None with empty string\n",
    "for c in df_token_lists:\n",
    "    if str(df_token_lists[c].dtype) in ('object', 'string_', 'unicode_'):\n",
    "        df_token_lists[c].fillna(value='', inplace=True)\n",
    "\n",
    "df_lem_strings = pd.DataFrame(li_lem_strings, columns=['lem quote'])\n",
    "\n",
    "print()\n",
    "print(\"\")\n",
    "print(\"Tokenized, stemed and lemmatized first 5 entries:\")\n",
    "print(df_lem_strings.head(5).to_string())\n",
    "\n",
    "## tokeniezed words df = df_token_lists\n",
    "## tokeniezed entry df = df_lem_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              0            1             2             3             4  \\\n",
      "0  construction       safety         blitz            on          both   \n",
      "1           cbd    apartment  construction          shut          down   \n",
      "2  construction           of    continuing          care      facility   \n",
      "3   frightening         drop            in  construction        safety   \n",
      "4           how  inspections           can       improve  construction   \n",
      "\n",
      "             5          6         7             8         9  ... 18 19 20 21  \\\n",
      "0        sides         of       the        border  safework  ...               \n",
      "1         over  minefield                      of    safety  ...               \n",
      "2       halted        due        to        safety  concerns  ...               \n",
      "3  inspections    exposed     unite           the     union  ...               \n",
      "4         site     safety  canadian  occupational    safety  ...               \n",
      "\n",
      "  22 23 24 25 26 rec_id  \n",
      "0                     1  \n",
      "1                     2  \n",
      "2                     3  \n",
      "3                     4  \n",
      "4                     5  \n",
      "\n",
      "[5 rows x 28 columns]\n",
      "(9600, 28)\n",
      "   rec_id tokenized_word\n",
      "0       1   construction\n",
      "1       2            cbd\n",
      "2       3   construction\n",
      "3       4    frightening\n",
      "4       5            how\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(118323, 2)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokeniezed words df = df_token_lists\n",
    "import numpy as np\n",
    "df_token_lists['rec_id'] = np.arange(1, df_token_lists.shape[0] + 1)\n",
    "\n",
    "print(df_token_lists.head())\n",
    "\n",
    "print (df_token_lists.shape) \n",
    "\n",
    "df_token_lists_unpivoted = df_token_lists.melt(id_vars=['rec_id'], var_name='word_order', value_name='tokenized_word').drop(['word_order'], axis=1)\n",
    "\n",
    "df_token_lists_unpivoted = df_token_lists_unpivoted[df_token_lists_unpivoted['tokenized_word'].str.strip().astype(bool)]\n",
    "\n",
    "print(df_token_lists_unpivoted.head())\n",
    "df_token_lists_unpivoted.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df2 shape: (49023, 4)\n"
     ]
    }
   ],
   "source": [
    "## Reference list to token, stem version, lem version and the POS tag,\n",
    "df_all_words = pd.DataFrame(li_tokens, columns=['token', 'stem', 'lem', 'pos'])\n",
    "print (f'df2 shape: {df_all_words.shape}') #(49023, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group by lemmatized words, add count and sort:\n",
      "Get just the first row in each lemmatized group\n",
      "Top 10 lemmatized words within all entries:\n",
      "            lem  index         token          stem pos  counts\n",
      "0  safetyhealth     85  safetyhealth  safetyhealth  NN     900\n",
      "1         start    296       started         start  VB     300\n",
      "2          fine    253         fines          fine  NN     200\n",
      "3         death     56        deaths         death  NN     200\n",
      "4    technology    117  technologies     technolog  NN     200\n",
      "5          halt      8        halted          halt  VB     200\n",
      "6        boston    125        boston        boston  NN     200\n",
      "7    contractor     87   contractors    contractor  NN     200\n",
      "8       satnews    322       satnews        satnew  NN     200\n",
      "9          fall    121       falling          fall  VB     200\n",
      "(506, 6)\n"
     ]
    }
   ],
   "source": [
    "# Add counts by pos from all words with POS, 'NN' is also the default value when POS not found\n",
    "print(\"Group by lemmatized words, add count and sort:\")\n",
    "df_all_words['counts'] = df_all_words.groupby(['lem'])['lem'].transform('count')\n",
    "df_all_words = df_all_words.sort_values(by=['counts', 'lem'], ascending=[False, True]).reset_index()\n",
    "\n",
    "print(\"Get just the first row in each lemmatized group\")\n",
    "df_words = df_all_words.groupby('lem').first().sort_values(by='counts', ascending=False).reset_index()\n",
    "print(\"Top 10 lemmatized words within all entries:\")\n",
    "print(df_words.head(10))\n",
    "print (f'All lem word with POS data size : {df_words.shape}.') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS tags within all abstracts\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NN</td>\n",
       "      <td>415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>VB</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JJ</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RB</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  pos  count\n",
       "0  NN    415\n",
       "1  VB     50\n",
       "2  JJ     34\n",
       "3  RB      7"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('POS tags within all abstracts')\n",
    "df_words.groupby(['pos']).size().sort_values(ascending=False).reset_index(name='count')\n",
    "## has 4 types of POS across all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed duplicates shape: (49023, 6)\n",
      "All token words list shape: (5732963, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rec_id</th>\n",
       "      <th>tokenized_word</th>\n",
       "      <th>index</th>\n",
       "      <th>stem</th>\n",
       "      <th>lem</th>\n",
       "      <th>pos</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>construction</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>cbd</td>\n",
       "      <td>2.0</td>\n",
       "      <td>cbd</td>\n",
       "      <td>cbd</td>\n",
       "      <td>NN</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>cbd</td>\n",
       "      <td>490.0</td>\n",
       "      <td>cbd</td>\n",
       "      <td>cbd</td>\n",
       "      <td>NN</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>cbd</td>\n",
       "      <td>978.0</td>\n",
       "      <td>cbd</td>\n",
       "      <td>cbd</td>\n",
       "      <td>NN</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>cbd</td>\n",
       "      <td>1466.0</td>\n",
       "      <td>cbd</td>\n",
       "      <td>cbd</td>\n",
       "      <td>NN</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rec_id tokenized_word   index stem  lem  pos  counts\n",
       "0       1   construction     NaN  NaN  NaN  NaN     NaN\n",
       "1       2            cbd     2.0  cbd  cbd   NN   100.0\n",
       "2       2            cbd   490.0  cbd  cbd   NN   100.0\n",
       "3       2            cbd   978.0  cbd  cbd   NN   100.0\n",
       "4       2            cbd  1466.0  cbd  cbd   NN   100.0"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create reference list with raw entry record number 'rec_id','NN' is also the default value when POS not found\n",
    "\n",
    "### drop duplucate tokens\n",
    "df_all_words2 = df_all_words.drop_duplicates()\n",
    "print (f'Removed duplicates shape: {df_all_words2.shape}') \n",
    "df_all_words2.head()\n",
    "\n",
    "## join with df_token_lists by token to get the rec_id which is the record id in the raw data\n",
    "df_all_token_words = df_token_lists_unpivoted.merge(df_all_words2, how = 'left', left_on='tokenized_word',right_on ='token').drop(['token'], axis=1)\n",
    "\n",
    "print (f'All token words list shape: {df_all_token_words.shape}')\n",
    "df_all_token_words.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "## count words by each entries to see if can summaries the entry a bit more \n",
    "df_all_token_words_entry = df_all_token_words.groupby(['rec_id','lem','pos']).size().sort_values(ascending=False).reset_index(name='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rec_id</th>\n",
       "      <th>lem</th>\n",
       "      <th>pos</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8243</td>\n",
       "      <td>safetyhealth</td>\n",
       "      <td>NN</td>\n",
       "      <td>1800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>553</td>\n",
       "      <td>safetyhealth</td>\n",
       "      <td>NN</td>\n",
       "      <td>1800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7562</td>\n",
       "      <td>safetyhealth</td>\n",
       "      <td>NN</td>\n",
       "      <td>1800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2470</td>\n",
       "      <td>safetyhealth</td>\n",
       "      <td>NN</td>\n",
       "      <td>1800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3154</td>\n",
       "      <td>safetyhealth</td>\n",
       "      <td>NN</td>\n",
       "      <td>1800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48089</th>\n",
       "      <td>3264</td>\n",
       "      <td>constructconnectcom</td>\n",
       "      <td>NN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48090</th>\n",
       "      <td>3264</td>\n",
       "      <td>commercial</td>\n",
       "      <td>JJ</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48091</th>\n",
       "      <td>2880</td>\n",
       "      <td>constructconnectcom</td>\n",
       "      <td>NN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48092</th>\n",
       "      <td>2880</td>\n",
       "      <td>commercial</td>\n",
       "      <td>JJ</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48093</th>\n",
       "      <td>2816</td>\n",
       "      <td>plea</td>\n",
       "      <td>NN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48094 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       rec_id                  lem pos  count\n",
       "0        8243         safetyhealth  NN   1800\n",
       "1         553         safetyhealth  NN   1800\n",
       "2        7562         safetyhealth  NN   1800\n",
       "3        2470         safetyhealth  NN   1800\n",
       "4        3154         safetyhealth  NN   1800\n",
       "...       ...                  ...  ..    ...\n",
       "48089    3264  constructconnectcom  NN      2\n",
       "48090    3264           commercial  JJ      2\n",
       "48091    2880  constructconnectcom  NN      2\n",
       "48092    2880           commercial  JJ      2\n",
       "48093    2816                 plea  NN      2\n",
       "\n",
       "[48094 rows x 4 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_token_words_entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first entry\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rec_id</th>\n",
       "      <th>lem</th>\n",
       "      <th>pos</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3721</th>\n",
       "      <td>1</td>\n",
       "      <td>border</td>\n",
       "      <td>NN</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8133</th>\n",
       "      <td>1</td>\n",
       "      <td>side</td>\n",
       "      <td>NN</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      rec_id     lem pos  count\n",
       "3721       1  border  NN    100\n",
       "8133       1    side  NN    100"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## words within first entry, not able to summaries by the highest number of each POS, as there are many 'NN' with the same count\n",
    "print('first entry')\n",
    "df_all_token_words_entry_1 = df_all_token_words_entry[df_all_token_words_entry['rec_id']==1].sort_values('count',ascending=False)\n",
    "df_all_token_words_entry_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rec_id</th>\n",
       "      <th>lem</th>\n",
       "      <th>pos</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3721</th>\n",
       "      <td>1</td>\n",
       "      <td>border</td>\n",
       "      <td>NN</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      rec_id     lem pos  count\n",
       "3721       1  border  NN    100"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## get the lem with latest count of the pos, only 4 words, not able to summaries the contains\n",
    "df_all_token_words_entry_1.loc[df_all_token_words_entry_1.groupby(['rec_id','pos'])['count'].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    lemmatized entry  rec_id\n",
      "0                      - - - - - side - - border - -       1\n",
      "1  cbd - - shut - - minefield - - - - illawarra m...       2\n",
      "2                  - - - care - halt due - - - - - -       3\n",
      "3         frighten drop - - - - expose unite - union       4\n",
      "4                                - - - - - - - - - -       5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9600, 2)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokeniezed entry df = df_lem_strings\n",
    "df_lem_strings['rec_id'] = np.arange(1, df_lem_strings.shape[0] + 1)\n",
    "df_lem_strings.rename(columns={ df_lem_strings.columns[0]: 'lemmatized entry'}, inplace=True)\n",
    "\n",
    "print(df_lem_strings.head())\n",
    "df_lem_strings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data size: (9600, 6)\n",
      "lem_string data size: (9600, 2)\n",
      "tokenised word data size: (5732963, 7)\n"
     ]
    }
   ],
   "source": [
    "print(f'Original data size: {df.shape}')\n",
    "print(f'lem_string data size: {df_lem_strings.shape}')\n",
    "print(f'tokenised word data size: {df_all_token_words.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5732963, 13)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>source</th>\n",
       "      <th>published_date</th>\n",
       "      <th>abstract</th>\n",
       "      <th>rec_id</th>\n",
       "      <th>lemmatized entry</th>\n",
       "      <th>tokenized_word</th>\n",
       "      <th>index</th>\n",
       "      <th>stem</th>\n",
       "      <th>lem</th>\n",
       "      <th>pos</th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Construction safety blitz on both sides of the...</td>\n",
       "      <td>{'href': 'https://www.safework.nsw.gov.au', 't...</td>\n",
       "      <td>2022-08-16 17:00:00+10:00</td>\n",
       "      <td>Construction safety blitz on both sides of the...</td>\n",
       "      <td>1</td>\n",
       "      <td>- - - - - side - - border - -</td>\n",
       "      <td>construction</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Construction safety blitz on both sides of the...</td>\n",
       "      <td>{'href': 'https://www.safework.nsw.gov.au', 't...</td>\n",
       "      <td>2022-08-16 17:00:00+10:00</td>\n",
       "      <td>Construction safety blitz on both sides of the...</td>\n",
       "      <td>1</td>\n",
       "      <td>- - - - - side - - border - -</td>\n",
       "      <td>safety</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Construction safety blitz on both sides of the...</td>\n",
       "      <td>{'href': 'https://www.safework.nsw.gov.au', 't...</td>\n",
       "      <td>2022-08-16 17:00:00+10:00</td>\n",
       "      <td>Construction safety blitz on both sides of the...</td>\n",
       "      <td>1</td>\n",
       "      <td>- - - - - side - - border - -</td>\n",
       "      <td>blitz</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>Construction safety blitz on both sides of the...</td>\n",
       "      <td>{'href': 'https://www.safework.nsw.gov.au', 't...</td>\n",
       "      <td>2022-08-16 17:00:00+10:00</td>\n",
       "      <td>Construction safety blitz on both sides of the...</td>\n",
       "      <td>1</td>\n",
       "      <td>- - - - - side - - border - -</td>\n",
       "      <td>on</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Construction safety blitz on both sides of the...</td>\n",
       "      <td>{'href': 'https://www.safework.nsw.gov.au', 't...</td>\n",
       "      <td>2022-08-16 17:00:00+10:00</td>\n",
       "      <td>Construction safety blitz on both sides of the...</td>\n",
       "      <td>1</td>\n",
       "      <td>- - - - - side - - border - -</td>\n",
       "      <td>both</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0           1  Construction safety blitz on both sides of the...   \n",
       "1           1  Construction safety blitz on both sides of the...   \n",
       "2           1  Construction safety blitz on both sides of the...   \n",
       "3           1  Construction safety blitz on both sides of the...   \n",
       "4           1  Construction safety blitz on both sides of the...   \n",
       "\n",
       "                                              source  \\\n",
       "0  {'href': 'https://www.safework.nsw.gov.au', 't...   \n",
       "1  {'href': 'https://www.safework.nsw.gov.au', 't...   \n",
       "2  {'href': 'https://www.safework.nsw.gov.au', 't...   \n",
       "3  {'href': 'https://www.safework.nsw.gov.au', 't...   \n",
       "4  {'href': 'https://www.safework.nsw.gov.au', 't...   \n",
       "\n",
       "             published_date  \\\n",
       "0 2022-08-16 17:00:00+10:00   \n",
       "1 2022-08-16 17:00:00+10:00   \n",
       "2 2022-08-16 17:00:00+10:00   \n",
       "3 2022-08-16 17:00:00+10:00   \n",
       "4 2022-08-16 17:00:00+10:00   \n",
       "\n",
       "                                            abstract  rec_id  \\\n",
       "0  Construction safety blitz on both sides of the...       1   \n",
       "1  Construction safety blitz on both sides of the...       1   \n",
       "2  Construction safety blitz on both sides of the...       1   \n",
       "3  Construction safety blitz on both sides of the...       1   \n",
       "4  Construction safety blitz on both sides of the...       1   \n",
       "\n",
       "                lemmatized entry tokenized_word index stem lem pos counts  \n",
       "0  - - - - - side - - border - -   construction                            \n",
       "1  - - - - - side - - border - -         safety                            \n",
       "2  - - - - - side - - border - -          blitz                            \n",
       "3  - - - - - side - - border - -             on                            \n",
       "4  - - - - - side - - border - -           both                            "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output 2 - raw data with tokenize words for PBI word counts visual, and lemeatized entry for PBI word cloud, all join with rec_id\n",
    "from functools import reduce\n",
    "\n",
    "dfs = [df,df_lem_strings,df_all_token_words]\n",
    "\n",
    "df_merged2 = reduce(lambda  left,right: pd.merge(left,right,on=['rec_id'],\n",
    "                                            how='outer'), dfs).fillna('')\n",
    "\n",
    "df_merged2.to_csv(raw_name+'_df_merge_list.csv', index=True)\n",
    "print(f'The combinded file data size: {df_merged2.shape}.') ##(14439, 9)\n",
    "df_merged2.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
