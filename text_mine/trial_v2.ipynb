{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps:\n",
    "1) read in file\n",
    "2) custom stop list\n",
    "2.1) prefix words from pubmed ['conclusions', 'introduction', 'methods', 'purpose', 'results']\n",
    "2.2) remove words by frequency (not apply yet, still looking for abnormality)\n",
    "3) remove stop words, remove numberic, remove puntuation\n",
    "4) steming\n",
    "5) Apply POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1) get .csv directly from github URL\n",
    "import requests\n",
    "import io\n",
    "url = \"https://raw.githubusercontent.com/ivanutsmdsi/iLab1/william/Output/pubmed_data100.csv\"\n",
    "s=requests.get(url).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import pubmed_data.csv---\n",
    "import pandas as pd\n",
    "df=pd.read_csv(io.StringIO(s.decode('utf-8')))\n",
    "import numpy as np\n",
    "df['rec_id'] = np.arange(1, df.shape[0] + 1)\n",
    "## subset the data with abstract column only\n",
    "text = df['Abstract']\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    rec_id                                              Title  \\\n",
      "0        1           The rodeo athlete: sport science: part I   \n",
      "1        2  Utilization of personal protective equipment a...   \n",
      "2        3  Preventive Effects of Safety Helmets on Trauma...   \n",
      "3        4  The effects of sleep on workplace cognitive fa...   \n",
      "4        5  A review of the revised Functional Capacity In...   \n",
      "..     ...                                                ...   \n",
      "95      96  Formulating a programme of repairs to structur...   \n",
      "96      97  Framing and blaming: construction of workplace...   \n",
      "97      98       Fracture Liaison Services: the UK experience   \n",
      "98      99  Experts' perspectives on the application of pu...   \n",
      "99     100  Home-related injuries among under-five-year ch...   \n",
      "\n",
      "                                              Journal Publication Date  \\\n",
      "0                    Sports medicine (Auckland, N.Z.)       2010 May 1   \n",
      "1                                   BMC public health      2020 May 27   \n",
      "2   International journal of environmental researc...      2016 Oct 29   \n",
      "3           Journal of occupational health psychology         2019 Aug   \n",
      "4                                              Injury         2017 Mar   \n",
      "..                                                ...              ...   \n",
      "95                  Accident; analysis and prevention         2013 Aug   \n",
      "96  International journal of occupational and envi...     2013 Oct-Dec   \n",
      "97  Osteoporosis international : a journal establi...         2011 Aug   \n",
      "98  Chinese journal of traumatology = Zhonghua chu...         2020 Jun   \n",
      "99  International journal of injury control and sa...         2017 Sep   \n",
      "\n",
      "    Cited by                                           Abstract  \n",
      "0        NaN  Based on the tradition, history and lore of th...  \n",
      "1        NaN  Background:                       Personal pro...  \n",
      "2        NaN  Introduction:                       Work-relat...  \n",
      "3        NaN  Healthy employee sleep is important for occupa...  \n",
      "4        NaN  The measurement of functional outcomes followi...  \n",
      "..       ...                                                ...  \n",
      "95       NaN  Home injuries are a substantial health burden ...  \n",
      "96       NaN  Background:                       Legislators ...  \n",
      "97       NaN  Fracture Liaison Services (FLS) have been show...  \n",
      "98       NaN  Purpose:                       Successful appl...  \n",
      "99       NaN  This cross-sectional study was conducted in ru...  \n",
      "\n",
      "[100 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "from pydoc import describe\n",
    "describe(df)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   rec_id Publication Date                                              Title  \\\n",
      "0       1       2010 May 1           The rodeo athlete: sport science: part I   \n",
      "1       2      2020 May 27  Utilization of personal protective equipment a...   \n",
      "2       3      2016 Oct 29  Preventive Effects of Safety Helmets on Trauma...   \n",
      "3       4         2019 Aug  The effects of sleep on workplace cognitive fa...   \n",
      "4       5         2017 Mar  A review of the revised Functional Capacity In...   \n",
      "\n",
      "                                            Abstract  \n",
      "0  Based on the tradition, history and lore of th...  \n",
      "1  Background:                       Personal pro...  \n",
      "2  Introduction:                       Work-relat...  \n",
      "3  Healthy employee sleep is important for occupa...  \n",
      "4  The measurement of functional outcomes followi...  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100, 4)"
      ]
     },
     "execution_count": 337,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## subset the data with abstract column only\n",
    "df2 = df[['rec_id','Publication Date','Title','Abstract']]\n",
    "\n",
    "print(df2.head())\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\TinaM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\TinaM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\TinaM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\TinaM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\TinaM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "## 2. Create stopword list\n",
    "#2) Prep file\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('omw-1.4') # use in stemming\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 188 words in the default stop words list.\n",
      "{'out', 'than', 'conclusions:', 'introduction:', 'will', 'needn', 'results:', 'hasn', 'purpose:', 'they', \"shouldn't\", 'then', \"hadn't\", 'which', \"didn't\", 'the', 'in', 'methods:', 'before', 'own', \"mightn't\", \"haven't\", 'aren', 'it', 'doesn', 'theirs', 'down', 'himself', \"it's\", 'haven', 'is', 'again', 'above', 'have', 'up', 'them', 'this', 'had', 'does', 'its', 'why', 'yourself', 'was', 'or', 'these', 'won', 'has', 'so', 'do', 'wouldn', \"you're\", 'over', 'as', 'd', 'during', 'don', 'from', 'some', 'too', 'isn', \"won't\", 'when', 'ours', 'both', 'we', 'themselves', 'about', \"she's\", 'and', \"needn't\", 'myself', 'she', 'm', 'what', 'more', 'o', 'you', 'at', 'here', 'most', \"shan't\", 'wasn', 'ma', 'no', 'that', \"wouldn't\", 'until', 'her', 'hadn', 'my', 'each', 'just', 'how', 'ourselves', 'been', 'any', 'those', 'once', 'such', \"doesn't\", 'yourselves', \"that'll\", 'weren', \"weren't\", \"you'd\", 'his', 'hers', 'be', 'mustn', 'your', \"couldn't\", 'i', 'for', 'me', 'their', 'am', 'all', 'whom', 'against', 'herself', 'an', \"isn't\", 'can', 'did', 'further', 'br', 'few', 're', 'a', 'because', 'by', 'should', \"should've\", 'on', 's', 'into', 'to', 'between', 'after', 'him', 'are', 'only', 'if', 'he', 'where', 'now', 'through', 'other', 'very', 'objective:', \"you've\", 'not', 'didn', 'y', 'below', 'itself', 'of', 'were', \"you'll\", 'but', 'being', 'background:', 'off', 'nor', 'there', \"wasn't\", \"mustn't\", 'couldn', 'same', \"hasn't\", 'with', 'while', \"aren't\", 'doing', \"don't\", 'shan', 'yours', 'under', 'having', 't', 've', 'our', 'href', 'who', 'ain', 'mightn', 'shouldn', 'll'}\n"
     ]
    }
   ],
   "source": [
    "# 2.1) view the default stopword list:\n",
    "stops = set(stopwords.words('english'))\n",
    "\n",
    "pub_med_cust_stop = ['conclusions:', 'introduction:', 'methods:', 'purpose:', 'results:', 'objective:','background:']\n",
    "\n",
    "stops.update(['br', 'href'],pub_med_cust_stop)\n",
    "\n",
    "print(f'There are {len(stops)} words in the default stop words list.')\n",
    "print(stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term: \"objective:\" in the stopword list is: True\n"
     ]
    }
   ],
   "source": [
    "termsToChk = 'objective:'\n",
    "print(f'Term: \"{termsToChk}\" in the stopword list is: {termsToChk in stops}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.2) Create another customer stopword list from the word frequency\n",
    "## from sklearn.feature_extraction.text import CountVectorizer\n",
    "## from sklearn.feature_extraction.text import TfidfTransformer\n",
    "## from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TinaM\\AppData\\Local\\Temp/ipykernel_18664/1758994536.py:2: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  df2['Abstract'] = df2['Abstract'].str.replace(r'[0-9]+', '')\n",
      "C:\\Users\\TinaM\\AppData\\Local\\Temp/ipykernel_18664/1758994536.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['Abstract'] = df2['Abstract'].str.replace(r'[0-9]+', '')\n"
     ]
    }
   ],
   "source": [
    "# 3.1) remove numberic numbers\n",
    "df2['Abstract'] = df2['Abstract'].str.replace(r'[0-9]+', '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\TinaM\\AppData\\Local\\Temp/ipykernel_18664/580077175.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2['Abstract']=df2['Abstract'].apply(lambda words: ' '.join(word.lower() for word in words.split() if word not in stops))\n"
     ]
    }
   ],
   "source": [
    "# 3.2) remove stopwords\n",
    "df2['Abstract']=df2['Abstract'].apply(lambda words: ' '.join(word.lower() for word in words.split() if word not in stops))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample of the subset df\n",
      "   rec_id Publication Date                                              Title  \\\n",
      "0       1       2010 May 1           The rodeo athlete: sport science: part I   \n",
      "1       2      2020 May 27  Utilization of personal protective equipment a...   \n",
      "2       3      2016 Oct 29  Preventive Effects of Safety Helmets on Trauma...   \n",
      "3       4         2019 Aug  The effects of sleep on workplace cognitive fa...   \n",
      "4       5         2017 Mar  A review of the revised Functional Capacity In...   \n",
      "\n",
      "                                            Abstract  \n",
      "0  based tradition, history lore american west, w...  \n",
      "1  background: personal protective equipment (ppe...  \n",
      "2  introduction: work-related traumatic brain inj...  \n",
      "3  healthy employee sleep important occupational ...  \n",
      "4  the measurement functional outcomes following ...  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(100, 4)"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('sample of the subset df')\n",
    "print(df2.head())\n",
    "df2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Steming\n",
    "stops = stops\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "# POS (Parts Of Speech) for: NN = nouns, JJ = adjectives, VB= verbs and RB= adverbs, JJR = adjective, comparative (larger), WRB=wh- adverb (how), WP = wh- pronoun (who), WDT = wh-determiner (that, what)\n",
    "DI_POS_TYPES = {'NN':'n', 'JJ':'a', 'VB':'v', 'RB':'r', 'JJR':'c', 'WRB':'b','WP':'p','WDT':'t'} \n",
    "POS_TYPES = list(DI_POS_TYPES.keys())\n",
    "\n",
    "# Constraints on tokens\n",
    "MIN_STR_LEN = 3\n",
    "RE_VALID = '[a-zA-Z]' ## only keep alphabet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of 100 entries.\n",
      "The first entry of abstracts\n",
      "based tradition, history lore american west, well individualistic nature lifestyle sport rodeo, rodeo athlete achieved iconic status sport, literature, art entertainment. for half century, rodeo become staple organized sport programmes high schools, universities international competitions. the origins rodeo grew ranch work dating back spanish vaqueros s. the sport officially organized and, s, championships determined sport rodeo surpassed baseball auto racing spectator attendance. since then, sponsorship grown, resulting extensive worldwide popularity major media outlets. despite growing popularity, investigations exist regarding scientific aspects sport. rodeo competition activity basically intermittent nature, short periods highly intense activity. when considering experience and, thus, improvement rodeo achieved solely constant punishing practices involving actual repetitive, human versus livestock competition, practices closely imitate sport-specific form interval training. studies, address anthropometric performance characteristics rodeo competitors, reveal comparable athletes traditional sports. the psychological constructs conducive performance rodeo varied limited, research efforts focused personality characteristics, sensation seeking competitive anxiety. nevertheless, evaluated relative higher levels traditional sport performance, rodeo participants closely resemble mainstream counterparts. although efforts quantify non-traditional sport still initial stages, information concerning optimal fitness level rodeo athletes maximal performance levels, basically anaerobic sport, remains determined area future study. rodeo performance, sports, based multifactorial array variables and, therefore, interdisciplinary efforts encompassing expertise across medicine, science coaching encouraged. taking comprehensive approach assessment athletes, well development quantification event-specific training protocols, may ultimately enhance athletic potential, minimize opportunity injury possibly provide information coaches allied health professionals appropriate development optimal medical care athletes.\n",
      "The second entry of abstracts\n",
      "background: personal protective equipment (ppe) material, device, equipment, clothing used worn workers reduce chance exposure contact harmful material energy causes injury, disease, even death. the use ppe universal legal requirement reduce occupational injuries illnesses workplace. therefore, study conducted assess ppe utilization associated factors among building construction workers addis ababa, ethiopia, . methods: institution based quantitative cross-sectional study conducted selected construction sites addis ababa city april may , . data collected among () building construction workers via interviewer-administered questionnaires. data entered epi info version . exported spss version statistical software analysis. variables p-value less . bivariate analysis included multivariate logistic regression. finally, variables p-value less . multivariate analysis considered significantly associated. results: the utilization least one ppe among building construction workers addis ababa found %. the majority (.%) participants' reason using ppe unavailability ppe followed absence orientation using ppe (.%). the majority (.%) (.%) participants knew abrasion type injury suffered abrasion respectively. factors associated utilization ppe presence training ppe use (aor = .; % ci: ., .), presence safety training (aor = ,; % ci:., .), safety orientation commencing work (aor = .; % ci:., .) presence supervision (aor = .; % ci:.,). conclusions: ppe utilization among building construction workers addis ababa low. the main reasons non-utilization ppe unavailability materials absence orientation using ppe. the presence ppe use training, presence safety training, safety orientation, governmental supervision factors associated ppe utilization. there continuous supervision construction sites assure workers get material training use it.\n"
     ]
    }
   ],
   "source": [
    "# convcert the text column to analysis to a list\n",
    "abstracts= df2.Abstract.tolist()\n",
    "\n",
    "print(f'Total of {len(abstracts)} entries.')\n",
    "\n",
    "print('The first entry of abstracts')\n",
    "print(abstracts[0])\n",
    "\n",
    "print('The second entry of abstracts')\n",
    "print(abstracts[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.1) set up functions to remove accents\n",
    "import unicodedata ## for remove accents function\n",
    "import string ## for remove accents function\n",
    "import re ## to remove character not in RE_VALID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## function from https://colab.research.google.com/github/gal-a/blog/blob/master/docs/notebooks/nlp/nltk_preprocess.ipynb#scrollTo=-44aMwUcQZxm\n",
    "# Remove accents function\n",
    "def remove_accents(data):\n",
    "    return ''.join(x for x in unicodedata.normalize('NFKD', data) if x in string.ascii_letters or x == \" \")\n",
    "\n",
    "# Process all quotes\n",
    "li_tokens = []\n",
    "li_token_lists = []\n",
    "li_lem_strings = []\n",
    "\n",
    "for i,text in enumerate(abstracts):\n",
    "    # Tokenize by sentence, then by lowercase word\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "\n",
    "    # Process all tokens per quote\n",
    "    li_tokens_quote = []\n",
    "    li_tokens_quote_lem = []\n",
    "    for token in tokens:\n",
    "        # Remove accents\n",
    "        t = remove_accents(token)\n",
    "\n",
    "        # Remove punctuation\n",
    "        t = str(t).translate(string.punctuation)\n",
    "        li_tokens_quote.append(t)\n",
    "        \n",
    "        # Add token that represents \"no lemmatization match\"\n",
    "        li_tokens_quote_lem.append(\"-\") # this token will be removed if a lemmatization match is found below\n",
    "\n",
    "        # Process each token\n",
    "        if t not in stops:\n",
    "            if re.search(RE_VALID, t):\n",
    "                if len(t) >= MIN_STR_LEN:\n",
    "                    # Note that the POS (Part Of Speech) is necessary as input to the lemmatizer \n",
    "                    # (otherwise it assumes the word is a noun)\n",
    "                    pos = nltk.pos_tag([t])[0][1][:2]\n",
    "                    pos2 = 'n'  # set default to noun\n",
    "                    if pos in DI_POS_TYPES:\n",
    "                      pos2 = DI_POS_TYPES[pos]\n",
    "                    \n",
    "                    stem = stemmer.stem(t)\n",
    "                    lem = lemmatizer.lemmatize(t, pos=pos2)  # lemmatize with the correct POS\n",
    "                    \n",
    "                    if pos in POS_TYPES:\n",
    "                        li_tokens.append((t, stem, lem, pos))\n",
    "\n",
    "                        # Remove the \"-\" token and append the lemmatization match\n",
    "                        li_tokens_quote_lem = li_tokens_quote_lem[:-1] \n",
    "                        li_tokens_quote_lem.append(lem)\n",
    "\n",
    "    # Build list of token lists from lemmatized tokens\n",
    "    li_token_lists.append(li_tokens_quote)\n",
    "    \n",
    "    # Build list of strings from lemmatized tokens\n",
    "    str_li_tokens_quote_lem = ' '.join(li_tokens_quote_lem)\n",
    "    li_lem_strings.append(str_li_tokens_quote_lem)\n",
    "    \n",
    "# Build resulting dataframes from lists\n",
    "df_token_lists = pd.DataFrame(li_token_lists) # (100, 549)\n",
    "\n",
    "print(\"Tokenized words in the first 5 entries:\")\n",
    "print(df_token_lists.head(5).to_string())\n",
    "\n",
    "# Replace None with empty string\n",
    "for c in df_token_lists:\n",
    "    if str(df_token_lists[c].dtype) in ('object', 'string_', 'unicode_'):\n",
    "        df_token_lists[c].fillna(value='', inplace=True)\n",
    "\n",
    "df_lem_strings = pd.DataFrame(li_lem_strings, columns=['lem quote'])\n",
    "\n",
    "print()\n",
    "print(\"\")\n",
    "print(\"Tokenized, stemed and lemmatized first 5 entries:\")\n",
    "print(df_lem_strings.head(5).to_string())\n",
    "\n",
    "## tokeniezed words df = df_token_lists\n",
    "## tokeniezed entry df = df_lem_strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              0            1            2           3             4         5  \\\n",
      "0         based    tradition                  history          lore  american   \n",
      "1    background                  personal  protective     equipment             \n",
      "2  introduction               workrelated   traumatic         brain    injury   \n",
      "3       healthy     employee        sleep   important  occupational    safety   \n",
      "4           the  measurement   functional    outcomes     following    severe   \n",
      "\n",
      "        6           7           8                9  ... 558 559 560 561 562  \\\n",
      "0    west                    well  individualistic  ...                       \n",
      "1     ppe                material                   ...                       \n",
      "2                 tbi                       caused  ...                       \n",
      "3          mechanisms     explain    relationships  ...                       \n",
      "4  trauma      widely  recognised         priority  ...                       \n",
      "\n",
      "  563 564 565 566 rec_id  \n",
      "0                      1  \n",
      "1                      2  \n",
      "2                      3  \n",
      "3                      4  \n",
      "4                      5  \n",
      "\n",
      "[5 rows x 568 columns]\n",
      "(100, 568)\n",
      "   rec_id tokenized_word\n",
      "0       1          based\n",
      "1       2     background\n",
      "2       3   introduction\n",
      "3       4        healthy\n",
      "4       5            the\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(15199, 2)"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokeniezed words df = df_token_lists\n",
    "import numpy as np\n",
    "df_token_lists['rec_id'] = np.arange(1, df_token_lists.shape[0] + 1)\n",
    "\n",
    "print(df_token_lists.head())\n",
    "\n",
    "print (df_token_lists.shape) # (100, 550)\n",
    "\n",
    "df_token_lists_unpivoted = df_token_lists.melt(id_vars=['rec_id'], var_name='word_order', value_name='tokenized_word').drop(['word_order'], axis=1)\n",
    "\n",
    "df_token_lists_unpivoted = df_token_lists_unpivoted[df_token_lists_unpivoted['tokenized_word'].str.strip().astype(bool)]\n",
    "\n",
    "print(df_token_lists_unpivoted.head())\n",
    "df_token_lists_unpivoted.shape # (14439, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df2 shape: (13935, 4)\n"
     ]
    }
   ],
   "source": [
    "## Reference list to token, stem version, lem version and the POS tag,\n",
    "df_all_words = pd.DataFrame(li_tokens, columns=['token', 'stem', 'lem', 'pos'])\n",
    "print (f'df2 shape: {df_all_words.shape}') #(13770, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group by lemmatized words, add count and sort:\n",
      "Get just the first row in each lemmatized group\n",
      "Top 10 lemmatized words within all entries:\n",
      "            lem  level_0  index         token       stem pos  counts\n",
      "0        injury        0    208        injury     injuri  NN     419\n",
      "1           use      419    231          used        use  VB     143\n",
      "2         study      562    108       studies      studi  NN     140\n",
      "3  construction      702    265  construction  construct  NN     111\n",
      "4        result      813     58     resulting     result  VB     111\n",
      "5        worker      924    233       workers     worker  NN      96\n",
      "6          risk     1020    667          risk       risk  NN      95\n",
      "7       patient     1115    453      patients    patient  NN      94\n",
      "8        safety     1209    361        safety     safeti  NN      93\n",
      "9          data     1302    284          data       data  NN      77\n",
      "(3195, 7)\n"
     ]
    }
   ],
   "source": [
    "# Add counts by pos from all words with POS, 'NN' is also the default value when POS not found\n",
    "print(\"Group by lemmatized words, add count and sort:\")\n",
    "df_all_words['counts'] = df_all_words.groupby(['lem'])['lem'].transform('count')\n",
    "df_all_words = df_all_words.sort_values(by=['counts', 'lem'], ascending=[False, True]).reset_index()\n",
    "\n",
    "print(\"Get just the first row in each lemmatized group\")\n",
    "df_words = df_all_words.groupby('lem').first().sort_values(by='counts', ascending=False).reset_index()\n",
    "print(\"Top 10 lemmatized words within all entries:\")\n",
    "print(df_words.head(10))\n",
    "print (df_words.shape) # (3195, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pos</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NN</td>\n",
       "      <td>2189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>JJ</td>\n",
       "      <td>429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>VB</td>\n",
       "      <td>418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RB</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  pos  count\n",
       "0  NN   2189\n",
       "1  JJ    429\n",
       "2  VB    418\n",
       "3  RB    159"
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_words.groupby(['pos']).size().sort_values(ascending=False).reset_index(name='count')\n",
    "## has 4 types of POS across all words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create reference list with raw entry record number 'rec_id','NN' is also the default value when POS not found\n",
    "\n",
    "### drop duplucate tokens\n",
    "df_all_words2 = df_all_words.drop_duplicates()\n",
    "print (f'Removed duplicates shape: {df_all_words2.shape}') #(3862, 4)\n",
    "df_all_words2.head()\n",
    "\n",
    "## join with df_token_lists by token to get the rec_id which is the record id in the raw data\n",
    "df_all_token_words = df_token_lists_unpivoted.merge(df_all_words2, how = 'left', left_on='tokenized_word',right_on ='token').drop(['token'], axis=1)\n",
    "\n",
    "print (f'All token words list shape: {df_all_token_words.shape}')\n",
    "df_all_token_words.head() # 14439 rows × 6 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## count words by each entries to see if can summaries the entry a bit more \n",
    "df_all_token_words_entry = df_all_token_words.groupby(['rec_id','lem','pos']).size().sort_values(ascending=False).reset_index(name='count')\n",
    "# or #df.count_column=df.groupby(['col5','col2']).col5.transform('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## words within first entry, not able to summaries by the highest number of each POS, as there are many 'NN' with the same count\n",
    "print('first entry')\n",
    "df_all_token_words_entry_1 = df_all_token_words_entry[df_all_token_words_entry['rec_id']==1].sort_values('count',ascending=False)\n",
    "df_all_token_words_entry_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get the lem with latest count of the pos, only 4 words, not able to summaries the contains\n",
    "df_all_token_words_entry_1.loc[df_all_token_words_entry_1.groupby(['rec_id','pos'])['count'].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokeniezed entry df = df_lem_strings\n",
    "df_lem_strings['rec_id'] = np.arange(1, df_lem_strings.shape[0] + 1)\n",
    "df_lem_strings.rename(columns={ df_lem_strings.columns[0]: 'lemmatized entry'}, inplace=True)\n",
    "\n",
    "print(df_lem_strings.head())\n",
    "df_lem_strings.shape # (100, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output 1 - raw data with tokenize words for PBI word counts visual, and lemeatized entry for PBI word cloud, all join with rec_id\n",
    "from functools import reduce\n",
    "\n",
    "dfs = [df2,df_lem_strings,df_all_token_words.drop(['count'], axis=1)]\n",
    "\n",
    "df_merged2 = reduce(lambda  left,right: pd.merge(left,right,on=['rec_id'],\n",
    "                                            how='outer'), dfs).fillna('')\n",
    "\n",
    "df_merged2.to_csv('df_merge_list.csv', index=True)\n",
    "print(df_merged2.shape) ##(14439, 9)\n",
    "df_merged2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3195, 3)"
      ]
     },
     "execution_count": 355,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_words.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words per type of POS within all entries\n",
      "\n",
      "POS_TYPE: NN\n",
      "             lem pos  counts\n",
      "0         injury  NN     419\n",
      "2          study  NN     140\n",
      "3   construction  NN     111\n",
      "5         worker  NN      96\n",
      "6           risk  NN      95\n",
      "7        patient  NN      94\n",
      "8         safety  NN      93\n",
      "9           data  NN      77\n",
      "10          rate  NN      66\n",
      "11        factor  NN      65\n",
      "\n",
      "POS_TYPE: JJ\n",
      "             lem pos  counts\n",
      "13          high  JJ      61\n",
      "25  occupational  JJ      44\n",
      "33   significant  JJ      38\n",
      "40         total  JJ      32\n",
      "41        common  JJ      31\n",
      "52     different  JJ      29\n",
      "54       injured  JJ      29\n",
      "70           low  JJ      26\n",
      "74     potential  JJ      25\n",
      "75           due  JJ      25\n",
      "\n",
      "POS_TYPE: VB\n",
      "         lem pos  counts\n",
      "1        use  VB     143\n",
      "4     result  VB     111\n",
      "22  identify  VB      48\n",
      "23   include  VB      46\n",
      "27    reduce  VB      43\n",
      "37   prevent  VB      33\n",
      "38      base  VB      33\n",
      "39  evaluate  VB      33\n",
      "46  increase  VB      30\n",
      "53    follow  VB      29\n",
      "\n",
      "POS_TYPE: RB\n",
      "               lem pos  counts\n",
      "51   significantly  RB      29\n",
      "58            also  RB      28\n",
      "76            well  RB      25\n",
      "138        however  RB      17\n",
      "231        elderly  RB      12\n",
      "240    potentially  RB      11\n",
      "258          first  RB      11\n",
      "259           less  RB      11\n",
      "274      currently  RB      10\n",
      "322          early  RB       9\n",
      "\n",
      "POS_TYPE: JJR\n",
      "Empty DataFrame\n",
      "Columns: [lem, pos, counts]\n",
      "Index: []\n",
      "\n",
      "POS_TYPE: WRB\n",
      "Empty DataFrame\n",
      "Columns: [lem, pos, counts]\n",
      "Index: []\n",
      "\n",
      "POS_TYPE: WP\n",
      "Empty DataFrame\n",
      "Columns: [lem, pos, counts]\n",
      "Index: []\n",
      "\n",
      "POS_TYPE: WDT\n",
      "Empty DataFrame\n",
      "Columns: [lem, pos, counts]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "# Top 10 words per Part Of Speech (POS), this for view only, in PBI can generate from df_words direclty\n",
    "\n",
    "print('Top 10 words per type of POS within all entries')\n",
    "df_words = df_words[['lem', 'pos', 'counts']]\n",
    "for v in POS_TYPES:\n",
    "    df_pos = df_words[df_words['pos'] == v]\n",
    "   \n",
    "    print()\n",
    "    print(\"POS_TYPE:\", v)\n",
    "    #print(df_pos.head(10).to_string())\n",
    "    print(df_pos.head(10))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
